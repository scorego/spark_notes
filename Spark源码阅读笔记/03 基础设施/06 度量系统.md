> 对于一个系统而言，首先考虑要满足一些业务场景，并实现功能。随着系统功能越来越多，代码量级越来越高，系统的可维护性、可测试性、性能都会成为新的挑战，这时监控功能就变得越来越重要了。在Facebook，更是把功能、日志以及监控列为同等重要，作为一个合格工程师的三驾马车。
>
> Spark作为优秀的开源系统，在监控方面也有自己的一整套体系。Spark的度量系统使用codahale提供的Metrics。Metrics作为一款监控指标的度量类库，可以为第三方库提供辅助统计信息，还可以将度量数据发送给Ganglia和Graphite以提供图形化的监控。

# 一、 Metrics

官网：https://metrics.dropwizard.io/

Github：https://github.com/dropwizard/metrics

官方介绍：Capturing JVM- and application-level metrics. So you know what's going on.

`Metrics`作为一款监控指标的度量类库，提供了许多工具帮助开发者来完成自定义的监控工作。

## 1. `MetricRegistry`

`Metrics`中`MetricRegistry`是中心容器，它是程序中所有度量的容器，所有新的度量工具都要注册到一个`MetricRegistry`实例中才可以使用，尽量在一个应用中保持让这个`MetricRegistry`实例保持单例。

## 2. 度量类型

Metrics提供多种度量类型：`Meters`、`Gauges`、`Counters`、`Histograms`、`Timers`等。

- `Meter`本身是一种只能自增的计数器，通常用来度量一系列事件发生的速率，如TPS。它提供平均速率、指数平滑平均速率、以及采样后的1分钟/5分钟/15分钟的速率。
- `Gauge`是一个最简单的计量，一般用来统计瞬时状态的数据信息。可以通过`Gauges`完成自定义的度量类型。
- `Counter`本质是`AtomicLong`，维护一个计数器，可以通过`inc()`和`dec()`方法对计数器做修改，用来计数。
- `Histograms`直方图，主要使用来统计数据的分布情况。提供最大值、最小值、平均值、中位数，百分比等，可以自定义绘制直方图。
- `Timer`是一个`Meter`和`Histogram`的组合。这个度量单位可以比较方便地统计请求的速率和处理时间。
- `HealthChecks`健康检查，用于对系统应用、子模块、关联模块的运行是否正常做检测。

## 3. 报告`Reporter`

`Metrics`通过报表，将采集的数据展现到不同的位置。比如注册一个`ConsoleReporter`到`MetricRegistry`中，那么console中就会打印出对应的信息。除此之外`Metrics`还支持`JMX`、`HTTP`、`Slf4j`等等，可以访问官网来查看`Metrics`提供的报表，如果还是不能满足自己的业务，也可以自己继承`Metrics`提供的`ScheduledReporter`类完成自定义的报表类。

# 二、 Spark度量系统

Spark的度量系统中有三个概念：

- `Instance`：指定了度量系统的实例名。Spark按照`Instance`的不同，区分为Master、Worker、Application、Driver和Executor。

- `Source`：指定了从哪里收集度量数据，即度量数据的来源。
- `Sink`：指定了往哪里输出度量数据，即度量数据的输出。Spark中使用`MetricsServlet`作为默认的Sink，此外还提供了`ConsoleSink`、`CsvSink`、`JmxSink`、`GraphiteSink`等实现。

## 1. `Source`

```scala
package org.apache.spark.metrics.source

private[spark] trait Source {
    // 度量源的名称
    def sourceName: String
    
    // 当前度量源的注册表
    def metricRegistry: MetricRegistry
}
```

Spark提供了应用的度量来源（`ApplicationSource`）、Worker的度量来源（`WorkerSource`）、DAGScheduler的度量来源（`DAGSchedulerSource`）、BlockManager的度量来源（`BlockManagerSource`）等诸多实现，对各个服务或组件进行监控。

```
AccumulatorSource (org.apache.spark.metrics.source)
ApplicationSource (org.apache.spark.deploy.master)
AppStatusSource (org.apache.spark.status)
BlockManagerSource (org.apache.spark.storage)
CacheMetrics (org.apache.spark.deploy.history)
CodegenMetrics$ (org.apache.spark.metrics.source)
DAGSchedulerSource (org.apache.spark.scheduler)
ExecutorAllocationManagerSource in ExecutorAllocationManager (org.apache.spark)
ExecutorMetricsSource (org.apache.spark.executor)
ExecutorSource (org.apache.spark.executor)
ExternalShuffleServiceSource (org.apache.spark.deploy)
HiveCatalogMetrics$ (org.apache.spark.metrics.source)
JVMCPUSource (org.apache.spark.metrics.source)
JvmSource (org.apache.spark.metrics.source)
LiveListenerBusMetrics (org.apache.spark.scheduler)
MasterSource (org.apache.spark.deploy.master)
PluginMetricsSource in PluginContextImpl (org.apache.spark.internal.plugin)
ShuffleMetricsSource in BlockManager$ (org.apache.spark.storage)
StreamingSource (org.apache.spark.streaming)
WorkerSource (org.apache.spark.deploy.worker)
```

以`ApplicationSource`为例，这是监控应用程序的：

```scala
package org.apache.spark.deploy.master

private[master] class ApplicationSource(val application: ApplicationInfo) extends Source {
    override val metricRegistry = new MetricRegistry()
    override val sourceName = "%s.%s.%s".format("application", application.desc.name, System.currentTimeMillis())

    // 应用状态，包括：WAITING, RUNNING, FINISHED, FAILED, KILLED, UNKNOWN
    metricRegistry.register(MetricRegistry.name("status"), new Gauge[String] {
        override def getValue: String = application.state.toString
    })
	
    // 运行持续时长
    metricRegistry.register(MetricRegistry.name("runtime_ms"), new Gauge[Long] {
        override def getValue: Long = application.duration
    })
	
    // 授权的内核数
    metricRegistry.register(MetricRegistry.name("cores"), new Gauge[Int] {
        override def getValue: Int = application.coresGranted
    })

}
```

## 2. `Sink`

 `Source`准备好度量数据后，就需要考虑如何输出和使用的问题。

```scala
package org.apache.spark.metrics.sink

private[spark] trait Sink {
    // 启动Sink
    def start(): Unit
    
    // 停止Sink
    def stop(): Unit
    
    // 输出到目的地
    def report(): Unit
}
```

继承类有：

```
ConsoleSink (org.apache.spark.metrics.sink)
CsvSink (org.apache.spark.metrics.sink)
GraphiteSink (org.apache.spark.metrics.sink)
JmxSink (org.apache.spark.metrics.sink)
MetricsServlet (org.apache.spark.metrics.sink)
PrometheusServlet (org.apache.spark.metrics.sink)
Slf4jSink (org.apache.spark.metrics.sink)
StatsdSink (org.apache.spark.metrics.sink)
```

以`Slf4jSink`为例：

```scala
package org.apache.spark.metrics.sink

private[spark] class Slf4jSink(val property: Properties,
                               val registry: MetricRegistry,
                               securityMgr: SecurityManager) extends Sink {
    val SLF4J_DEFAULT_PERIOD = 10
    val SLF4J_DEFAULT_UNIT = "SECONDS"

    val SLF4J_KEY_PERIOD = "period"
    val SLF4J_KEY_UNIT = "unit"

    val pollPeriod = Option(property.getProperty(SLF4J_KEY_PERIOD)) match {
        case Some(s) => s.toInt
        case None => SLF4J_DEFAULT_PERIOD
    }

    val pollUnit: TimeUnit = Option(property.getProperty(SLF4J_KEY_UNIT)) match {
        case Some(s) => TimeUnit.valueOf(s.toUpperCase(Locale.ROOT))
        case None => TimeUnit.valueOf(SLF4J_DEFAULT_UNIT)
    }

    MetricsSystem.checkMinimalPollingPeriod(pollUnit, pollPeriod)

    val reporter: Slf4jReporter = Slf4jReporter.forRegistry(registry)
        .convertDurationsTo(TimeUnit.MILLISECONDS)
        .convertRatesTo(TimeUnit.SECONDS)
        .build()

    override def start(): Unit = {
        reporter.start(pollPeriod, pollUnit)
    }

    override def stop(): Unit = {
        reporter.stop()
    }

    override def report(): Unit = {
        reporter.report()
    }
}
```

可以看出，`Slf4jSink`的`start()`、`stop()`及`report()`实际都是代理了Metrics库中的`Slf4jReporter`的相关方法。而`start()`方法传递的两个参数`pollPeriod`和`pollUnit`用来作为定时器获取数据的周期和时间单位。

## 3. `MetricSystem`

`MetricSystem`是对外的一个服务，可以用来注册`source`/`sink`，以及启动`sink`。

```scala
package org.apache.spark.metrics

/**
 * Spark Metrics System, created by a specific "instance", combined by source,
 * sink, periodically polls source metrics data to sink destinations.
 *
 * "instance" specifies "who" (the role) uses the metrics system. In Spark, there are several roles
 * like master, worker, executor, client driver. These roles will create metrics system
 * for monitoring. So, "instance" represents these roles. Currently in Spark, several instances
 * have already implemented: master, worker, executor, driver, applications.
 *
 * "source" specifies "where" (source) to collect metrics data from. In metrics system, there exists
 * two kinds of source:
 *   1. Spark internal source, like MasterSource, WorkerSource, etc, which will collect
 *   Spark component's internal state, these sources are related to instance and will be
 *   added after a specific metrics system is created.
 *   2. Common source, like JvmSource, which will collect low level state, is configured by
 *   configuration and loaded through reflection.
 *
 * "sink" specifies "where" (destination) to output metrics data to. Several sinks can
 * coexist and metrics can be flushed to all these sinks.
 *
 * Metrics configuration format is like below:
 * [instance].[sink|source].[name].[options] = xxxx
 *
 * [instance] can be "master", "worker", "executor", "driver", "applications" which means only
 * the specified instance has this property.
 * wild card "*" can be used to replace instance name, which means all the instances will have this property.
 *
 * [sink|source] means this property belongs to source or sink. This field can only be source or sink.
 *
 * [name] specify the name of sink or source, if it is custom defined.
 *
 * [options] represent the specific property of this source or sink.
 */
private[spark] class MetricsSystem private (val instance: String,
                                            conf: SparkConf,
                                            securityMgr: SecurityManager) extends Logging {...}
```

