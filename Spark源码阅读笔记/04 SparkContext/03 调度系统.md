> `org.apache.spark.scheduler`包最重要的三个类是：
>
> - `org.apache.spark.scheduler.SchedulerBackend`
> - `org.apache.spark.scheduler.TaskScheduler`
> - `org.apache.spark.scheduler.DAGScheduler`
>
> `TaskScheduler`负责请求集群管理器给应用程序分配并运行Executor（一级调度）、给任务分配Executor并运行任务（二级调度），可以看做是任务调度的客户端。`DAGScheduler`主要用于在任务正式交给`TaskSchedulerImpl`之前做一些准备工作，包括创建Job、将DAG中的RDD划分为不同的Stage、提交Stage等。
>
> `SparkContext`中初始化`SchedulerBackend`和` TaskScheduler`的代码如下：
>
> ```scala
> // Create and start the scheduler
>  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
>  _schedulerBackend = sched
>  _taskScheduler = ts
>  _dagScheduler = new DAGScheduler(this)
>  _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
> ```
>
> 

Spark资源调度分为两层：第一层是集群管理器将资源分配给Application；第二层是Application进一步将资源分配给各个Task。这里所说的调度系统指的是第二层，主要由`DAGScheduler`和`TaskScheduler`构成，工作流程如下：

![img](./pics/调度01_流程.png)

1. build operator DAG

   用户提交的Job将首先被转换为一系列RDD，并由RDD之间的依赖关系构建DAG，然后将DAG提交到调度系统。

2. split graph into stages of tasks

   `DAGScheduler`接收由RDD构成的DAG，将RDD划分到不同的Stage（类型有`ResultStage`和`ShuffleMapStage`）。根据Stage的不同类型，创建不同类型的Task（`ResultTask`和`ShuffleMapTask`）。每个Stage将因为未完成Partition的多少，创建零到多个Task。`DAGScheduler`最后将每个Stage中的Task以任务集合(`TaskSet`)的形式提交给`TaskScheduler`。

3. launch tasks via cluster manager

   使用集群管理器分配资源与任务调度，对于失败的任务有一定的重试和容错机制。`TaskScheduler`接收`TaskSet`，创建`TaskSetManager`对其进行管理，并将此`TaskSetManager`添加到调度池中，最后对Task的调度交给调度后端接口`SchedulerBackend`处理。`SchedulerBackend`首先申请`TaskScheduler`，按照调度算法（FIFO或者FAIR）对调度池中的所有`TaskSetManager`进行排序，然后对`TaskSet`按照最大本地性原则分配资源，最后在各个分配的节点上运行`TaskSet`中的Task。

4. execute tasks

   执行任务，并将任务的中间结果和最终结果存入存储体系。

# 一、 `DAGScheduler`

`SparkContext`中初始化调度器的代码如下：

```scala
val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
_dagScheduler = new DAGScheduler(this)
_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
```

在生成`DAGScheduler`之前，已经生成了`SchedulerBackend`和`TaskScheduler`对象。之所以`TaskScheduler`对象在`DAGScheduler`对象构造之前先生成，是由于在生成`DAGScheduler`的构造方法中会从传入的`SparkContext`中获取`TaskScheduler`对象。

```SCALA
package org.apache.spark.scheduler

private[spark] class DAGScheduler(
  	// 获得当前SparkContext对象
    private[scheduler] val sc: SparkContext,
  	// 获得当前saprkContext内置的taskScheduler
    private[scheduler] val taskScheduler: TaskScheduler,
  	// 异步处理事件的对象，从sc中获取
    listenerBus: LiveListenerBus,
  	//运行在Driver端管理shuffle map task的输出，从sc中获取
    mapOutputTracker: MapOutputTrackerMaster,
  	//运行在driver端，管理整个Job的Block信息，从sc中获取
    blockManagerMaster: BlockManagerMaster,
  	// 从sc中获取
    env: SparkEnv,
    clock: Clock = new SystemClock()) extends Logging {
    
  def this(sc: SparkContext, taskScheduler: TaskScheduler) = {
    this(
      sc,
      taskScheduler,
      sc.listenerBus,
      sc.env.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster],
      sc.env.blockManager.master,
      sc.env)
  }

  def this(sc: SparkContext) = this(sc, sc.taskScheduler)
	
  private[spark] val metricsSource: DAGSchedulerSource = new DAGSchedulerSource(this)
	
  // 用于生成JobId
  private[scheduler] val nextJobId = new AtomicInteger(0)
  // 总Job数
  private[scheduler] def numTotalJobs: Int = nextJobId.get()
  // 下一个StageId
  private val nextStageId = new AtomicInteger(0)
	
  // 记录某个job id对应包含的所有stage
  private[scheduler] val jobIdToStageIds = new HashMap[Int, HashSet[Int]]
  // 记录StageId对应的Stage
  private[scheduler] val stageIdToStage = new HashMap[Int, Stage]

  // shuffle dependency ID到ShuffleMapStage的映射
  private[scheduler] val shuffleIdToMapStage = new HashMap[Int, ShuffleMapStage]
  // 记录处于Active状态的job，key为jobId, value为ActiveJob类型对象
  private[scheduler] val jobIdToActiveJob = new HashMap[Int, ActiveJob]

  // 等待运行的Stage，一般这些是在等待Parent Stage运行完成才能开始
  private[scheduler] val waitingStages = new HashSet[Stage]

  // 处于Running状态的Stage
  private[scheduler] val runningStages = new HashSet[Stage]

  // 失败原因为fetch failures的Stage，等待重新提交
  private[scheduler] val failedStages = new HashSet[Stage]
	
  // active状态的Job列表
  private[scheduler] val activeJobs = new HashSet[ActiveJob]

  private val cacheLocs = new HashMap[Int, IndexedSeq[Seq[TaskLocation]]]
  
  private val executorFailureEpoch = new HashMap[String, Long]
  
  private val shuffleFileLostEpoch = new HashMap[String, Long]

  private [scheduler] val outputCommitCoordinator = env.outputCommitCoordinator

  // A closure serializer that we reuse
  private val closureSerializer = SparkEnv.get.closureSerializer.newInstance()

  /** If enabled, FetchFailed will not cause stage retry, in order to surface the problem. */
  private val disallowStageRetryForTest = sc.getConf.get(TEST_NO_STAGE_RETRY)

  private val shouldMergeResourceProfiles = sc.getConf.get(config.RESOURCE_PROFILE_MERGE_CONFLICTS)
  
  private[scheduler] val unRegisterOutputOnHostOnFetchFailure =
    sc.getConf.get(config.UNREGISTER_OUTPUT_ON_HOST_ON_FETCH_FAILURE)
  
  private[scheduler] val maxConsecutiveStageAttempts =
    sc.getConf.getInt("spark.stage.maxConsecutiveAttempts", DAGScheduler.DEFAULT_MAX_CONSECUTIVE_STAGE_ATTEMPTS)
  
  private[scheduler] val barrierJobIdToNumTasksCheckFailures = new ConcurrentHashMap[Int, Int]

  private val timeIntervalNumTasksCheck = sc.getConf.get(config.BARRIER_MAX_CONCURRENT_TASKS_CHECK_INTERVAL)
  
  private val maxFailureNumTasksCheck = sc.getConf.get(config.BARRIER_MAX_CONCURRENT_TASKS_CHECK_MAX_FAILURES)

  private val messageScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor("dag-scheduler-message")
	
  
  private[spark] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  taskScheduler.setDAGScheduler(this)

  private val pushBasedShuffleEnabled = Utils.isPushBasedShuffleEnabled(sc.getConf)
  
  ...
  
  // 构造完成时，会调用eventProcessLoop.start()方法，启动一个多线程，然后把各种event都提交到eventProcessLoop中
  eventProcessLoop.start()
}

private[spark] object DAGScheduler {
  
  val RESUBMIT_TIMEOUT = 200

  val DEFAULT_MAX_CONSECUTIVE_STAGE_ATTEMPTS = 4
}
```

## 1. 提交Job

一个Job实际上是在RDD上调用一个action操作开始的，action操作最终会调用`org.apache.spark.SparkContext#runJob`方法，在`SparkContext`中有多个重载的`runJob()`方法，最终入口是下面这个：

```scala
def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
  if (stopped.get()) {
    throw new IllegalStateException("SparkContext has been shutdown")
  }
  val callSite = getCallSite
  val cleanedFunc = clean(func)
  logInfo("Starting job: " + callSite.shortForm)
  if (conf.getBoolean("spark.logLineage", false)) {
    logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
  }
  // 这里调用org.apache.spark.scheduler.DAGScheduler#runJob方法
  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
  progressBar.foreach(_.finishAll())
  rdd.doCheckpoint()
}
```

最终，`org.apache.spark.SparkContext#runJob`会调用`org.apache.spark.scheduler.DAGScheduler#runJob`，从而被Spark调度。

### a. `org.apache.spark.scheduler.DAGScheduler#runJob`

代码如下：

```scala
def runJob[T, U](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    partitions: Seq[Int],
    callSite: CallSite,
    resultHandler: (Int, U) => Unit,
    properties: Properties): Unit = {
  val start = System.nanoTime
  //调用DAGScheduler#submitJob方法得到一个JobWaiter实例来监听Job的执行情况。
  val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
  ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf)
  // 针对Job的Succeeded状态和Failed状态，在接下来代码中都有不同的处理方式。
  waiter.completionFuture.value.get match {
    case scala.util.Success(_) =>
        logInfo("Job %d finished: %s, took %f s".format
                (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
    case scala.util.Failure(exception) =>
        logInfo("Job %d failed: %s, took %f s".format
                (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.
        val callerStackTrace = Thread.currentThread().getStackTrace.tail
        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)
        throw exception
  }
}
```

### b. `org.apache.spark.scheduler.DAGScheduler#submitJob`

代码如下：

```scala
def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
  
    // 首先检查rdd的分区信息，确保rdd分区信息正确
    val maxPartitions = rdd.partitions.length
    partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>
      throw new IllegalArgumentException(
        "Attempting to access a non-existent partition: " + p + ". " +
        "Total number of partitions: " + maxPartitions)
    }
		
  	// 给当前job生成一个id。在同一个SparkContext中，jobId由0开始，逐渐自增。
    val jobId = nextJobId.getAndIncrement()
  
    if (partitions.isEmpty) {
      val clonedProperties = Utils.cloneProperties(properties)
      if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) {
        clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm)
      }
      val time = clock.getTimeMillis()
      listenerBus.post(
        SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties))
      listenerBus.post(
        SparkListenerJobEnd(jobId, time, JobSucceeded))
      // Return immediately if the job is running 0 tasks
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }

    assert(partitions.nonEmpty)
  
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
  
  	// 构造出一个JobWaiter对象
    val waiter = new JobWaiter[U](this, jobId, partitions.size, resultHandler)
  
  	// 投递JobSubmitted事件，最终会调用DAGScheduler#handleJobSubmitted来处理job
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      Utils.cloneProperties(properties)))
  
    waiter
  }
```

### c. `org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted`

此方法开始处理Job，并执行`Stage`的划分。`Stage`的划分是从最后一个`Stage`开始逆推的，每遇到一个宽依赖处，就分裂成另外一个`Stage`，依此类推直到`Stage`划分完毕。并且，只有最后一个`Stage`的类型是`ResultStage`类型。

```scala
private[scheduler] def handleJobSubmitted(jobId: Int,
                                          finalRDD: RDD[_],
                                          func: (TaskContext, Iterator[_]) => _,
                                          partitions: Array[Int],
                                          callSite: CallSite,
                                          listener: JobListener,
                                          properties: Properties): Unit = {
  var finalStage: ResultStage = null
  try {
    // Stage划分过程是从最后一个Stage开始往前执行的，最后一个Stage的类型是ResultStage
    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
  } catch {
    case e: BarrierJobSlotsNumberCheckFailed =>
      // If jobId doesn't exist in the map, Scala coverts its value null to 0: Int automatically.
      val numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,
                                                                         (_: Int, value: Int) => value + 1)

      logWarning(s"Barrier stage in job $jobId requires ${e.requiredConcurrentTasks} slots, " +
                 s"but only ${e.maxConcurrentTasks} are available. " +
                 s"Will retry up to ${maxFailureNumTasksCheck - numCheckFailures + 1} more times")

      if (numCheckFailures <= maxFailureNumTasksCheck) {
        messageScheduler.schedule(
          new Runnable {
            override def run(): Unit = eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,
                                                                          partitions, callSite, 
                                                                          listener, properties))
          },
          timeIntervalNumTasksCheck,
          TimeUnit.SECONDS
        )
        return
      } else {
        // Job failed, clear internal data.
        barrierJobIdToNumTasksCheckFailures.remove(jobId)
        listener.jobFailed(e)
        return
      }

    case e: Exception =>
      logWarning("Creating new stage failed due to exception - job: " + jobId, e)
      listener.jobFailed(e)
      return
  }
  // Job submitted, clear internal data.
  barrierJobIdToNumTasksCheckFailures.remove(jobId)

  // 为该Job生成一个ActiveJob对象，并准备计算这个finalStage
  val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
  clearCacheLocs()
  logInfo("Got job %s (%s) with %d output partitions".format(
    job.jobId, callSite.shortForm, partitions.length))
  logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
  logInfo("Parents of final stage: " + finalStage.parents)
  logInfo("Missing parents: " + getMissingParentStages(finalStage))

  val jobSubmissionTime = clock.getTimeMillis()
  jobIdToActiveJob(jobId) = job // 该job进入active状态
  activeJobs += job
  finalStage.setActiveJob(job)
  // 得到当前job执行所需要的全部stageId，包括finalStage所依赖的stage，以及依赖stage的祖先stage
  val stageIds = jobIdToStageIds(jobId).toArray
  val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))
  
  // 向LiveListenerBus发送Job提交事件
  listenerBus.post(
    SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos,
                          Utils.cloneProperties(properties)))
  
  // 提交当前Stage，会先递归提交父stage
  submitStage(finalStage)
}
```

### d. `org.apache.spark.scheduler.DAGScheduler#createResultStage`

`org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted`会首先切分出最后一个`Stage`(类型为`ResultStage`)。

```scala
private def createResultStage(
      rdd: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      jobId: Int,
      callSite: CallSite): ResultStage = {
  
  // 返回第一个宽依赖的parent
  val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)
  val resourceProfile = mergeResourceProfilesForStage(resourceProfiles)
  checkBarrierStageWithDynamicAllocation(rdd)
  checkBarrierStageWithNumSlots(rdd, resourceProfile)
  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)
  // 最后一个Stage是ResultStage
  val parents = getOrCreateParentStages(shuffleDeps, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ResultStage(id, rdd, func, partitions, parents, jobId,
                              callSite, resourceProfile.id)
  stageIdToStage(id) = stage
  updateJobIdStageIdMaps(jobId, stage)
  stage
}
```

### e. `org.apache.spark.scheduler.DAGScheduler#submitStage`

`org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted`最后会进行`Stage`的提交，调用的是`submitStage()`方法。

```scala
private def submitStage(stage: Stage): Unit = {
  val jobId = activeJobForStage(stage)
  if (jobId.isDefined) {
    logDebug(s"submitStage($stage (name=${stage.name};" +
             s"jobs=${stage.jobIds.toSeq.sorted.mkString(",")}))")
    if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
      val missing = getMissingParentStages(stage).sortBy(_.id)
      logDebug("missing: " + missing)
      if (missing.isEmpty) {
        logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
        submitMissingTasks(stage, jobId.get)
      } else {
        for (parent <- missing) {
          submitStage(parent)
        }
        waitingStages += stage
      }
    }
  } else {
    abortStage(stage, "No active job for stage " + stage.id, None)
  }
}
```

