> `org.apache.spark.scheduler`包最重要的三个类是：
>
> - `org.apache.spark.scheduler.SchedulerBackend`
> - `org.apache.spark.scheduler.TaskScheduler`
> - `org.apache.spark.scheduler.DAGScheduler`
>
> `DAGScheduler`主要用于在任务正式交给`TaskSchedulerImpl`之前做一些准备工作，包括创建Job、将DAG中的RDD划分为不同的Stage、提交Stage等。`TaskScheduler`负责请求集群管理器给应用程序分配并运行Executor（一级调度）、给任务分配Executor并运行任务（二级调度），可以看做是任务调度的客户端。
>
> `SparkContext`中初始化`SchedulerBackend`和` TaskScheduler`的代码如下：
>
> ```scala
> // Create and start the scheduler
>  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
>  _schedulerBackend = sched
>  _taskScheduler = ts
>  _dagScheduler = new DAGScheduler(this)
>  _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
> ```
>
> 

Spark资源调度分为两层：第一层是集群管理器将资源分配给Application；第二层是Application进一步将资源分配给各个Task。这里所说的调度系统指的是第二层，主要由`DAGScheduler`和`TaskScheduler`构成，工作流程如下：

![img](./pics/调度01_流程.png)

1. build operator DAG

   用户提交的Job将首先被转换为一系列RDD，并由RDD之间的依赖关系构建DAG，然后将DAG提交到调度系统。

2. split graph into stages of tasks

   `DAGScheduler`接收由RDD构成的DAG，将RDD划分到不同的Stage（类型有`ResultStage`和`ShuffleMapStage`）。根据Stage的不同类型，创建不同类型的Task（`ResultTask`和`ShuffleMapTask`）。每个Stage将因为未完成Partition的多少，创建零到多个Task。`DAGScheduler`最后将每个Stage中的Task以任务集合(`TaskSet`)的形式提交给`TaskScheduler`。

3. launch tasks via cluster manager

   使用集群管理器分配资源与任务调度，对于失败的任务有一定的重试和容错机制。`TaskScheduler`接收`TaskSet`，创建`TaskSetManager`对其进行管理，并将此`TaskSetManager`添加到调度池中，最后对Task的调度交给调度后端接口`SchedulerBackend`处理。`SchedulerBackend`首先申请`TaskScheduler`，按照调度算法（FIFO或者FAIR）对调度池中的所有`TaskSetManager`进行排序，然后对`TaskSet`按照最大本地性原则分配资源，最后在各个分配的节点上运行`TaskSet`中的Task。

4. execute tasks

   执行任务，并将任务的中间结果和最终结果存入存储体系。

# 一、 `org.apache.spark.scheduler.DAGScheduler`

## 1. 初始化

`SparkContext`中初始化调度器的代码如下：

```scala
val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
_dagScheduler = new DAGScheduler(this)
_heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
```

在生成`DAGScheduler`之前，已经生成了`SchedulerBackend`和`TaskScheduler`对象。之所以`TaskScheduler`对象在`DAGScheduler`对象构造之前先生成，是由于在生成`DAGScheduler`的构造方法中会从传入的`SparkContext`中获取`TaskScheduler`对象。

```SCALA
package org.apache.spark.scheduler

private[spark] class DAGScheduler(
  	// 获得当前SparkContext对象
    private[scheduler] val sc: SparkContext,
  	// 获得当前saprkContext内置的taskScheduler
    private[scheduler] val taskScheduler: TaskScheduler,
  	// 异步处理事件的对象，从sc中获取
    listenerBus: LiveListenerBus,
  	//运行在Driver端管理shuffle map task的输出，从sc中获取
    mapOutputTracker: MapOutputTrackerMaster,
  	//运行在driver端，管理整个Job的Block信息，从sc中获取
    blockManagerMaster: BlockManagerMaster,
  	// 从sc中获取
    env: SparkEnv,
    clock: Clock = new SystemClock()) extends Logging {
    
  def this(sc: SparkContext, taskScheduler: TaskScheduler) = {
    this(
      sc,
      taskScheduler,
      sc.listenerBus,
      sc.env.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster],
      sc.env.blockManager.master,
      sc.env)
  }

  def this(sc: SparkContext) = this(sc, sc.taskScheduler)
	
  private[spark] val metricsSource: DAGSchedulerSource = new DAGSchedulerSource(this)
	
  // 用于生成JobId
  private[scheduler] val nextJobId = new AtomicInteger(0)
  // 总Job数
  private[scheduler] def numTotalJobs: Int = nextJobId.get()
  // 下一个StageId
  private val nextStageId = new AtomicInteger(0)
	
  // 记录某个job id对应包含的所有stage
  private[scheduler] val jobIdToStageIds = new HashMap[Int, HashSet[Int]]
  
  // 记录StageId对应的Stage
  private[scheduler] val stageIdToStage = new HashMap[Int, Stage]

  // shuffle dependency ID到ShuffleMapStage的映射
  private[scheduler] val shuffleIdToMapStage = new HashMap[Int, ShuffleMapStage]
  
  // 记录处于Active状态的job，key为jobId, value为ActiveJob类型对象
  private[scheduler] val jobIdToActiveJob = new HashMap[Int, ActiveJob]

  // 等待运行的Stage，一般这些是在等待Parent Stage运行完成才能开始
  private[scheduler] val waitingStages = new HashSet[Stage]

  // 处于Running状态的Stage
  private[scheduler] val runningStages = new HashSet[Stage]

  // 失败原因为fetch failures的Stage，等待重新提交
  private[scheduler] val failedStages = new HashSet[Stage]
	
  // active状态的Job列表
  private[scheduler] val activeJobs = new HashSet[ActiveJob]

  private val cacheLocs = new HashMap[Int, IndexedSeq[Seq[TaskLocation]]]
  
  private val executorFailureEpoch = new HashMap[String, Long]
  
  private val shuffleFileLostEpoch = new HashMap[String, Long]

  private [scheduler] val outputCommitCoordinator = env.outputCommitCoordinator

  // A closure serializer that we reuse
  private val closureSerializer = SparkEnv.get.closureSerializer.newInstance()

  /** If enabled, FetchFailed will not cause stage retry, in order to surface the problem. */
  private val disallowStageRetryForTest = sc.getConf.get(TEST_NO_STAGE_RETRY)

  private val shouldMergeResourceProfiles = sc.getConf.get(config.RESOURCE_PROFILE_MERGE_CONFLICTS)
  
  private[scheduler] val unRegisterOutputOnHostOnFetchFailure =
    sc.getConf.get(config.UNREGISTER_OUTPUT_ON_HOST_ON_FETCH_FAILURE)
  
  private[scheduler] val maxConsecutiveStageAttempts =
    sc.getConf.getInt("spark.stage.maxConsecutiveAttempts", DAGScheduler.DEFAULT_MAX_CONSECUTIVE_STAGE_ATTEMPTS)
  
  private[scheduler] val barrierJobIdToNumTasksCheckFailures = new ConcurrentHashMap[Int, Int]

  private val timeIntervalNumTasksCheck = sc.getConf.get(config.BARRIER_MAX_CONCURRENT_TASKS_CHECK_INTERVAL)
  
  private val maxFailureNumTasksCheck = sc.getConf.get(config.BARRIER_MAX_CONCURRENT_TASKS_CHECK_MAX_FAILURES)

  private val messageScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor("dag-scheduler-message")
	
  
  private[spark] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)
  taskScheduler.setDAGScheduler(this)

  private val pushBasedShuffleEnabled = Utils.isPushBasedShuffleEnabled(sc.getConf)
  
  ...
  
  // 构造完成时，会调用eventProcessLoop.start()方法，启动一个多线程，然后把各种event都提交到eventProcessLoop中
  eventProcessLoop.start()
}

private[spark] object DAGScheduler {
  
  val RESUBMIT_TIMEOUT = 200

  val DEFAULT_MAX_CONSECUTIVE_STAGE_ATTEMPTS = 4
}
```

## 2. 提交Job

一个Job实际上是在RDD上调用一个action操作开始的，action操作最终会调用`org.apache.spark.SparkContext#runJob`方法（RDD持有对`SparkContext`的引用），在`SparkContext`中有多个重载的`runJob()`方法，最终会调用`org.apache.spark.scheduler.DAGScheduler#runJob`，从而被Spark调度。

```
RDD的action算子 -->  SparkContext#runJob --> DAGScheduler#runJob 
				--> DAGScheduler#submitJob -->  DAGScheduler#handleJobSubmitted
```

### a. `DAGScheduler#runJob`

代码如下：

```scala
def runJob[T, U](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    partitions: Seq[Int],
    callSite: CallSite,
    resultHandler: (Int, U) => Unit,
    properties: Properties): Unit = {
  val start = System.nanoTime
  
  //调用DAGScheduler#submitJob方法得到一个JobWaiter实例来监听Job的执行情况。
  val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
  
  // 针对Job的Succeeded状态和Failed状态，在接下来代码中都有不同的处理方式。
  ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf)
  waiter.completionFuture.value.get match {
    case scala.util.Success(_) => logInfo(...)
    case scala.util.Failure(exception) => logInfo(...); ...; throw exception
  }
}
```

### b. `DAGScheduler#submitJob`

`DAGScheduler#submitJob`通过投递`JobSubmitted`来提交job，这个事件最终会被`DAGScheduler#handleJobSubmitted`处理。`DAGScheduler#submitJob`代码如下：

```scala
def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
  
    // 首先检查rdd的分区信息，确保rdd分区信息正确
    val maxPartitions = rdd.partitions.length
    partitions.find(p => p >= maxPartitions || p < 0).foreach { p => throw new IllegalArgumentException(...) }
		
  	// 给当前job生成一个id。在同一个SparkContext中，jobId由0开始，逐渐自增。
    val jobId = nextJobId.getAndIncrement()
  	
  	// if job is running 0 task, 直接return
    if (partitions.isEmpty) {
      val clonedProperties = Utils.cloneProperties(properties)
      if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) {
        clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm)
      }
      val time = clock.getTimeMillis()
      listenerBus.post(SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties))
      listenerBus.post(SparkListenerJobEnd(jobId, time, JobSucceeded))
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }

    assert(partitions.nonEmpty)
  
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
  
  	// 构造出一个JobWaiter对象
    val waiter = new JobWaiter[U](this, jobId, partitions.size, resultHandler)
  
  	// 投递JobSubmitted事件，最终会调用DAGScheduler#handleJobSubmitted来处理job
    eventProcessLoop.post(JobSubmitted(jobId, rdd, func2, 
                                       partitions.toArray, callSite, waiter,
                                       Utils.cloneProperties(properties)))
  
    waiter
  }
```

### c. `DAGScheduler#handleJobSubmitted`

投递`JobSubmitted`事件后，最终会路由到`handleJobSubmitted()`方法来处理该事件。此方法开始处理Job，并执行`Stage`的划分。`Stage`的划分是从最后一个`Stage`开始逆推的，每遇到一个宽依赖处，就分裂成另外一个`Stage`，依此类推直到`Stage`划分完毕。并且，只有最后一个`Stage`的类型是`ResultStage`类型。

```scala
private[scheduler] def handleJobSubmitted(jobId: Int,
                                          finalRDD: RDD[_],
                                          func: (TaskContext, Iterator[_]) => _,
                                          partitions: Array[Int],
                                          callSite: CallSite,
                                          listener: JobListener,
                                          properties: Properties): Unit = {
  var finalStage: ResultStage = null
  try {
    // Stage划分过程是从最后一个Stage开始往前执行的，最后一个Stage的类型是ResultStage
    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
  } catch {
    case e: BarrierJobSlotsNumberCheckFailed =>
      // If jobId doesn't exist in the map, Scala coverts its value null to 0: Int automatically.
      val numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,
                                                                         (_: Int, value: Int) => value + 1)

      logWarning(s"Barrier stage in job $jobId requires ${e.requiredConcurrentTasks} slots, " +
                 s"but only ${e.maxConcurrentTasks} are available. " +
                 s"Will retry up to ${maxFailureNumTasksCheck - numCheckFailures + 1} more times")

      if (numCheckFailures <= maxFailureNumTasksCheck) {
        messageScheduler.schedule(
          new Runnable {
            override def run(): Unit = eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,
                                                                          partitions, callSite, 
                                                                          listener, properties))
          },
          timeIntervalNumTasksCheck,
          TimeUnit.SECONDS
        )
        return
      } else {
        // Job failed, clear internal data.
        barrierJobIdToNumTasksCheckFailures.remove(jobId)
        listener.jobFailed(e)
        return
      }

    case e: Exception =>
      logWarning("Creating new stage failed due to exception - job: " + jobId, e)
      listener.jobFailed(e)
      return
  }
  // Job submitted, clear internal data.
  barrierJobIdToNumTasksCheckFailures.remove(jobId)

  // 根据最后一个调度阶段(finalStage)生成作业
  val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
  clearCacheLocs()
  logInfo(...)

  val jobSubmissionTime = clock.getTimeMillis()
  // 该job进入active状态
  jobIdToActiveJob(jobId) = job 
  activeJobs += job
  finalStage.setActiveJob(job)
  // 得到当前job执行所需要的全部stageId，包括finalStage所依赖的stage，以及依赖stage的祖先stage
  val stageIds = jobIdToStageIds(jobId).toArray
  val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))
  
  // 向LiveListenerBus发送Job提交事件
  listenerBus.post(SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos,
                          								Utils.cloneProperties(properties)))
  
  // 提交当前Stage，会先递归提交父stage
  submitStage(finalStage)
}
```

### d. `DAGScheduler#createResultStage`

`org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted`会首先切分出最后一个`Stage`(类型为`ResultStage`)。

```scala
private def createResultStage(
      rdd: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      jobId: Int,
      callSite: CallSite): ResultStage = {
  
  // 返回第一个宽依赖的parent
  val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)
  val resourceProfile = mergeResourceProfilesForStage(resourceProfiles)
  
  checkBarrierStageWithDynamicAllocation(rdd)
  checkBarrierStageWithNumSlots(rdd, resourceProfile)
  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)
  
  // 最后一个Stage是ResultStage
  val parents = getOrCreateParentStages(shuffleDeps, jobId)
  val id = nextStageId.getAndIncrement()
  val stage = new ResultStage(id, rdd, func, partitions, parents, jobId,
                              callSite, resourceProfile.id)
  stageIdToStage(id) = stage
  updateJobIdStageIdMaps(jobId, stage)
  stage
}
```

#### ① `DAGScheduler#getShuffleDependenciesAndResourceProfiles`

返回RDD的直接父shuffle dependecies和当前stage的ResourceProfiles。

```scala
/**
   * Returns shuffle dependencies that are immediate parents of the given RDD and the
   * ResourceProfiles associated with the RDDs for this stage.
   *
   * This function will not return more distant ancestors for shuffle dependencies. For example,
   * if C has a shuffle dependency on B which has a shuffle dependency on A:
   *
   * A <-- B <-- C
   *
   * calling this function with rdd C will only return the B <-- C dependency.
   *
   * This function is scheduler-visible for the purpose of unit testing.
   */
private[scheduler] def getShuffleDependenciesAndResourceProfiles(
  rdd: RDD[_]): (HashSet[ShuffleDependency[_, _, _]], HashSet[ResourceProfile]) = {
  val parents = new HashSet[ShuffleDependency[_, _, _]]
  val resourceProfiles = new HashSet[ResourceProfile]
  val visited = new HashSet[RDD[_]]
  val waitingForVisit = new ListBuffer[RDD[_]]
  waitingForVisit += rdd
  while (waitingForVisit.nonEmpty) {
    val toVisit = waitingForVisit.remove(0)
    if (!visited(toVisit)) {
      visited += toVisit
      Option(toVisit.getResourceProfile).foreach(resourceProfiles += _)
      toVisit.dependencies.foreach {
        case shuffleDep: ShuffleDependency[_, _, _] =>
        parents += shuffleDep
        case dependency =>
        waitingForVisit.prepend(dependency.rdd)
      }
    }
  }
  (parents, resourceProfiles)
}
```

#### ② DAGScheduler#getOrCreateParentStages

```scala
  /**
   * Get or create the list of parent stages for the given shuffle dependencies. The new
   * Stages will be created with the provided firstJobId.
   */
  private def getOrCreateParentStages(shuffleDeps: HashSet[ShuffleDependency[_, _, _]],
      firstJobId: Int): List[Stage] = {
    shuffleDeps.map { shuffleDep =>
      getOrCreateShuffleMapStage(shuffleDep, firstJobId)
    }.toList
  }

private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      case Some(stage) => stage
      case None =>
        // Create stages for all missing ancestor shuffle dependencies.
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          // See SPARK-13902 for more information.
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        // Finally, create a stage for the given shuffle dependency.
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }

 def createShuffleMapStage[K, V, C](shuffleDep: ShuffleDependency[K, V, C], jobId: Int): ShuffleMapStage = {
    val rdd = shuffleDep.rdd
    val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)
    val resourceProfile = mergeResourceProfilesForStage(resourceProfiles)
    checkBarrierStageWithDynamicAllocation(rdd)
    checkBarrierStageWithNumSlots(rdd, resourceProfile)
    checkBarrierStageWithRDDChainPattern(rdd, rdd.getNumPartitions)
   
    val numTasks = rdd.partitions.length
    val parents = getOrCreateParentStages(shuffleDeps, jobId)
    val id = nextStageId.getAndIncrement()
   
    val stage = new ShuffleMapStage(
      id, rdd, numTasks, parents, jobId, 
      rdd.creationSite, 
      shuffleDep, 
      mapOutputTracker,
      resourceProfile.id)

    stageIdToStage(id) = stage
    shuffleIdToMapStage(shuffleDep.shuffleId) = stage
    updateJobIdStageIdMaps(jobId, stage)

    if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
      logInfo(s"Registering RDD ${rdd.id} (${rdd.getCreationSite}) as input to " +
        s"shuffle ${shuffleDep.shuffleId}")
      mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)
    }
    stage
  }
```



### e. `DAGScheduler#submitStage`

`org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted`最后会进行`Stage`的提交，调用的是`submitStage()`方法。

```scala
private def submitStage(stage: Stage): Unit = {
  val jobId = activeJobForStage(stage)
  if (jobId.isDefined) {
    logDebug(...)
    if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
      val missing = getMissingParentStages(stage).sortBy(_.id)
      logDebug("missing: " + missing)
      if (missing.isEmpty) {
        // 没有未提交的父stage，这个时候提交tasks
        logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
        submitMissingTasks(stage, jobId.get)
      } else {
        // 有未提交的父stage，先提交父stage
        for (parent <- missing) {
          submitStage(parent)
        }
        waitingStages += stage
      }
    }
  } else {
    abortStage(stage, "No active job for stage " + stage.id, None)
  }
}
```

#### ① `submitMissingTasks`

在调度stage提交运行后，在`submitMissingTasks()`方法中会根据partition个数拆分成对应数量的任务，这些任务组成一个TaskSet提交到`TaskScheduler`进行处理。

```scala
 private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
    logDebug("submitMissingTasks(" + stage + ")")
    stage match {
      case sms: ShuffleMapStage if stage.isIndeterminate && !sms.isAvailable =>
        mapOutputTracker.unregisterAllMapOutput(sms.shuffleDep.shuffleId)
      case _ =>
    }

    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
    val properties = jobIdToActiveJob(jobId).properties
    addPySparkConfigsToProperties(stage, properties)

    runningStages += stage
    stage match {
      case s: ShuffleMapStage =>
        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
        if (pushBasedShuffleEnabled) {
          prepareShuffleServicesForShuffleMapStage(s)
        }
      case s: ResultStage =>
        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
    }
    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =>
          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =>
          partitionsToCompute.map { id =>
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
      }
    } catch {
      case NonFatal(e) =>
        stage.makeNewStageAttempt(partitionsToCompute.size)
        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, Utils.cloneProperties(properties)))
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)

    if (partitionsToCompute.nonEmpty) {
      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
    }
    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, Utils.cloneProperties(properties)))

    var taskBinary: Broadcast[Array[Byte]] = null
    var partitions: Array[Partition] = null
    try {
      var taskBinaryBytes: Array[Byte] = null
      RDDCheckpointData.synchronized {
        taskBinaryBytes = stage match {
          case stage: ShuffleMapStage =>
            JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
          case stage: ResultStage =>
            JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
        }

        partitions = stage.rdd.partitions
      }

      if (taskBinaryBytes.length > TaskSetManager.TASK_SIZE_TO_WARN_KIB * 1024) {
        logWarning(s"Broadcasting large task binary with size " +
          s"${Utils.bytesToString(taskBinaryBytes.length)}")
      }
      taskBinary = sc.broadcast(taskBinaryBytes)
    } catch {
      case e: NotSerializableException =>
        abortStage(stage, "Task not serializable: " + e.toString, Some(e))
        runningStages -= stage
        return
      case e: Throwable =>
        abortStage(stage, s"Task serialization failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }

        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
    } catch {
      case NonFatal(e) =>
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    if (tasks.nonEmpty) {
      logInfo(...)
      // 将task以TaskSet的形式提交到taskScheduler
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,
        stage.resourceProfileId))
    } else {
      markStageAsFinished(stage, None)

      stage match {
        case stage: ShuffleMapStage => logDebug(...); markMapStageJobsAsFinished(stage)
        case stage : ResultStage => logDebug(...)
      }
      submitWaitingChildStages(stage)
    }
  }
```

# 二、`org.apache.spark.scheduler.TaskScheduler`

```scala
package org.apache.spark.scheduler

/**
 * Low-level task scheduler interface, currently implemented exclusively by
 * [[org.apache.spark.scheduler.TaskSchedulerImpl]].
 * This interface allows plugging in different task schedulers. Each TaskScheduler schedules tasks
 * for a single SparkContext. These schedulers get sets of tasks submitted to them from the
 * DAGScheduler for each stage, and are responsible for sending the tasks to the cluster, running
 * them, retrying if there are failures, and mitigating stragglers. They return events to the
 * DAGScheduler.
 */
private[spark] trait TaskScheduler {

  private val appId = "spark-application-" + System.currentTimeMillis

  def rootPool: Pool

  def schedulingMode: SchedulingMode

  def start(): Unit

  // Invoked after system has successfully initialized (typically in spark context).
  // Yarn uses this to bootstrap allocation of resources based on preferred locations,
  // wait for executor registrations, etc.
  def postStartHook(): Unit = { }

  // Disconnect from the cluster.
  def stop(): Unit

  // Submit a sequence of tasks to run.
  def submitTasks(taskSet: TaskSet): Unit

  // Kill all the tasks in a stage and fail the stage and all the jobs that depend on the stage.
  // Throw UnsupportedOperationException if the backend doesn't support kill tasks.
  def cancelTasks(stageId: Int, interruptThread: Boolean): Unit

  /**
   * Kills a task attempt.
   * Throw UnsupportedOperationException if the backend doesn't support kill a task.
   *
   * @return Whether the task was successfully killed.
   */
  def killTaskAttempt(taskId: Long, interruptThread: Boolean, reason: String): Boolean

  // Kill all the running task attempts in a stage.
  // Throw UnsupportedOperationException if the backend doesn't support kill tasks.
  def killAllTaskAttempts(stageId: Int, interruptThread: Boolean, reason: String): Unit

  // Notify the corresponding `TaskSetManager`s of the stage, that a partition has already completed
  // and they can skip running tasks for it.
  def notifyPartitionCompletion(stageId: Int, partitionId: Int): Unit

  // Set the DAG scheduler for upcalls. This is guaranteed to be set before submitTasks is called.
  def setDAGScheduler(dagScheduler: DAGScheduler): Unit

  // Get the default level of parallelism to use in the cluster, as a hint for sizing jobs.
  def defaultParallelism(): Int

  /**
   * Update metrics for in-progress tasks and executor metrics, and let the master know that the
   * BlockManager is still alive. Return true if the driver knows about the given block manager.
   * Otherwise, return false, indicating that the block manager should re-register.
   */
  def executorHeartbeatReceived(
      execId: String,
      accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
      blockManagerId: BlockManagerId,
      executorUpdates: Map[(Int, Int), ExecutorMetrics]): Boolean

  /**
   * Get an application ID associated with the job.
   *
   * @return An application ID
   */
  def applicationId(): String = appId

  /**
   * Process a decommissioning executor.
   */
  def executorDecommission(executorId: String, decommissionInfo: ExecutorDecommissionInfo): Unit

  /**
   * If an executor is decommissioned, return its corresponding decommission info
   */
  def getExecutorDecommissionState(executorId: String): Option[ExecutorDecommissionState]

  /**
   * Process a lost executor
   */
  def executorLost(executorId: String, reason: ExecutorLossReason): Unit

  /**
   * Process a removed worker
   */
  def workerRemoved(workerId: String, host: String, message: String): Unit

  /**
   * Get an application's attempt ID associated with the job.
   *
   * @return An application's Attempt ID
   */
  def applicationAttemptId(): Option[String]

}
```

