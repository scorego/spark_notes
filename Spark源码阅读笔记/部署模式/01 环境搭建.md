[toc]

> Spark有很多种部署模式：
>
> - 本地部署模式：`local`、`local[N]`、`local[*]`、`local[N, maxRetries]`
>
>   主要用于代码调试和跟踪，不具备高可用性、可扩展性，不适合生产环境
>
> - 本地集群部署模式：`local-cluster[N, cores, memory]`
>
>   主要用于代码调试和跟踪，常用于源码学习，不具备高可用性、可扩展性，不适合生产环境
>
> - `Standalone`模式：`spark://host:port`
>
>   Spark内部默认实现的一种集群管理模式，由集群的`Master`统一管理资源。具备高可用性，支持分布式部署，可用于生产环境。
>
> - 第三方部署模式：YARN client/cluster、`mesos://`、`zk://`、`simr://`等。
>
> 部署模式涉及很多角色：
>
> - `Application`
>
>   用户使用Spark API开发的应用程序，由一个`Driver`和一组运行于Spark集群上的`Executor`组成。
>
> - `Driver`
>
>   应用的驱动程序。
>
> - `Master`
>
>   Spark的主控节点。
>
> - `Worker`
>
>   Spark的工作节点。向`Master`汇报自身的资源、`Executor`执行状态的改变，并接受`Master`的命令启动`Executor`或`Driver`。
>
> - `Executor`
>
>   Spark的工作进程，由`Worker`监管，负责具体任务的执行。
>
> Spark在不同的部署模式下的区别主要是不同的`TaskScheduler` /`SchedulerBackend`实现。Spark会在`SparkContext`的创建过程中通过传入的Master URL来确定不同的部署模式，并且创建不同的`TaskScheduler`和`SchedulerBackend`，具体实现在`org.apache.spark.SparkContext#createTaskScheduler`中，不同的实现如下：
>
> | 部署模式                                                | `TaskScheduler`                              | `SchedulerBackend`                              |
> | :------------------------------------------------------ | -------------------------------------------- | ----------------------------------------------- |
> | `local`、`local[N]`、`local[*]`、`local[N, maxRetries]` | `TaskSchedulerImpl`                          | `LocalSchedulerBackend`                         |
> | `spark://host:port`、`local-cluster[N, cores, memory]`  | `TaskSchedulerImpl`                          | `StandaloneSchedulerBackend`                    |
> | other                                                   | `ExternalClusterManager#createTaskScheduler` | `ExternalClusterManager#createSchedulerBackend` |



# 一、 本地模式

## 1. 准备工作：

- JDK：可使用`java -version`查看java版本。
- scala环境：只需要开发环境有就行，部署环境并不是必需。
- Spark安装包：可在官网获取。

## 2. 原理

在本地使用多线程模拟分布式运行，每个线程代表一个worker。

Local部署模式的不同之处在于任务失败后的重试次数。

| Master-URL                                              | 备注                                                | Meaning                                                      | Max-Retry time               |
| :------------------------------------------------------ | --------------------------------------------------- | :----------------------------------------------------------- | :--------------------------- |
| local                                                   | 所有进程都运行在一台机器的JVM中                     | 1 worker Thread(no parallelism)                              | 失败的任务不会重新尝试       |
| local[N]                                                |                                                     | N  worker Threads,ideally set to cores number                | 失败的任务不会重新尝试       |
| local[*]                                                |                                                     | as many worker Threads as your machine                       | 失败的任务不会重新尝试       |
| local[N,F]                                              |                                                     | N worker Threads                                             | 失败的任务最多进行F-1次尝试  |
| local[*,F]                                              |                                                     | as many worker Threads as your machine                       | 失败的任务最多进行F-1次尝试  |
| local-cluster[numSlaves,coresPerSlave,memeoryPerySlave] | 在一台机器中模拟集群运行,相关的进程在同一台机器上。 | 伪分布式模式，本地运行master和worker，master中指定了worker数目，CPU core数目和每个worker能使用的内存 | 其他与standalone运行模式相同 |

## 3. 操作

可以先看下Spark安装包的目录结构：

```shell
[work@scorego spark-3.1.1-bin-hadoop2.7]$ pwd
/home/work/deploy/spark-3.1.1-bin-hadoop2.7
[work@scorego spark-3.1.1-bin-hadoop2.7]$ ll
总用量 156
drwxr-xr-x 2 work work  4096 6月   7 01:35 bin							脚本
drwxr-xr-x 2 work work  4096 6月   7 01:35 conf						配置文件
drwxr-xr-x 5 work work  4096 6月   7 01:35 data
drwxr-xr-x 4 work work  4096 6月   7 01:35 examples
drwxr-xr-x 2 work work 20480 6月   7 01:35 jars						jar包
drwxr-xr-x 4 work work  4096 6月   7 01:35 kubernetes
-rw-r--r-- 1 work work 23235 6月   7 01:35 LICENSE
drwxr-xr-x 2 work work  4096 6月   7 01:35 licenses
-rw-r--r-- 1 work work 57677 6月   7 01:35 NOTICE
drwxr-xr-x 7 work work  4096 6月   7 01:35 python
drwxr-xr-x 3 work work  4096 6月   7 01:35 R
-rw-r--r-- 1 work work  4488 6月   7 01:35 README.md
-rw-r--r-- 1 work work   183 6月   7 01:35 RELEASE
drwxr-xr-x 2 work work  4096 6月   7 01:35 sbin						脚本
drwxr-xr-x 2 work work  4096 6月   7 01:35 yarn
```

其实这时就已经可以使用local模式了。如果想不受目录限制随时使用相关命令，则可以把spark根目录添加到环境变量中：

```shell
vim /etc/profile
 
# 在文件末尾添加如下内容：
 
# SPARK_HOME
export SPARK_HOME=/home/work/deploy/spark-3.1.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
 
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.282.b08-1.el7_9.x86_64
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar

export SCALA_HOME=/home/work/tools/scala-2.12.13
export PATH=$PATH:$SCALA_HOME/bin
```

## 4. 测试

运行spark-shell:

![image-20210607014343636](./pics/03_001_spark-shell.png)

可以根据提示查看Web UI，地址为：http://180.76.156.161:4040/ (ip是云主机的地址。)



# 二、 Standalone 独立集群模式

## 1. 准备工作

## 2. 操作

### a. 集群规划

```
node1: master
node2: slave/worker
node3: slave/worker
```

### b. 配置worker

进入配置目录，修改`worker`配置文件（低版本中该配置文件名是`slaves`）：

```
cp workers.template worker
```

该配置文件的初始内容为：

```shell
# A Spark Worker will be started on each of the machines listed below.
localhost
```

修改为：

```bash
node1
node2
```

### c. 配置master

进入配置目录，修改`spark-env.sh`：

```shell
cp spark-env.sh.template spark-env.sh
```

添加内容：	

```
# 指定master的IP和端口
SPARK_MASTER_HOST=node1
SPARK_MASTER_PORT=7077

SPARK_MASTER_WEBUI_PORT=8080

# HADOOP配置文件目录
HADOOP_CONG_DIR=/home/work/deploy/hadoop2.7/conf
YARN_CONF_DIR=/home/work/deploy/hadoop2.7/conf

JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.282.b08-1.el7_9.x86_64
```

### d. 分发

把该Spark目录从master机器分发到另外的worker机器上。或者worker机器安装同版本Spark包，并修改配置文件也可。

```shell
# 用rsync之类的工具也可
cd /home/work/deploy
scp -r /home/work/deploy/spark-3.1.1-bin-hadoop2.7 worker@node2:$PWD
scp -r /home/work/deploy/spark-3.1.1-bin-hadoop2.7 worker@node3:$PWD
```



### 3. 集群的启动和停止

相关脚本在`sbin`目录：

```shell
[work@scorego spark-3.1.1-bin-hadoop2.7]$ ll sbin/
总用量 112
-rwxr-xr-x 1 work work  981 6月   7 01:35 decommission-slave.sh
-rwxr-xr-x 1 work work 2028 6月   7 01:35 decommission-worker.sh
-rwxr-xr-x 1 work work  957 6月   7 01:35 slaves.sh
-rwxr-xr-x 1 work work 1429 6月   7 01:35 spark-config.sh
-rwxr-xr-x 1 work work 6333 6月   7 01:35 spark-daemon.sh
-rwxr-xr-x 1 work work 1264 6月   7 01:35 spark-daemons.sh
-rwxr-xr-x 1 work work 1192 6月   7 01:35 start-all.sh
-rwxr-xr-x 1 work work 1764 6月   7 01:35 start-history-server.sh
-rwxr-xr-x 1 work work 2097 6月   7 01:35 start-master.sh
-rwxr-xr-x 1 work work 1877 6月   7 01:35 start-mesos-dispatcher.sh
-rwxr-xr-x 1 work work 1425 6月   7 01:35 start-mesos-shuffle-service.sh
-rwxr-xr-x 1 work work  967 6月   7 01:35 start-slave.sh					# deprecated, use start-worker.sh
-rwxr-xr-x 1 work work  969 6月   7 01:35 start-slaves.sh				# deprecated, use start-workers.sh
-rwxr-xr-x 1 work work 2025 6月   7 01:35 start-thriftserver.sh
-rwxr-xr-x 1 work work 3245 6月   7 01:35 start-worker.sh
-rwxr-xr-x 1 work work 1560 6月   7 01:35 start-workers.sh
-rwxr-xr-x 1 work work 1481 6月   7 01:35 stop-all.sh
-rwxr-xr-x 1 work work 1056 6月   7 01:35 stop-history-server.sh
-rwxr-xr-x 1 work work 1080 6月   7 01:35 stop-master.sh
-rwxr-xr-x 1 work work 1227 6月   7 01:35 stop-mesos-dispatcher.sh
-rwxr-xr-x 1 work work 1084 6月   7 01:35 stop-mesos-shuffle-service.sh
-rwxr-xr-x 1 work work  965 6月   7 01:35 stop-slave.sh				# deprecated, use stop-worker.sh
-rwxr-xr-x 1 work work  967 6月   7 01:35 stop-slaves.sh				# deprecated, use stop-workers.sh
-rwxr-xr-x 1 work work 1066 6月   7 01:35 stop-thriftserver.sh
-rwxr-xr-x 1 work work 1569 6月   7 01:35 stop-worker.sh
-rwxr-xr-x 1 work work 1066 6月   7 01:35 stop-workers.sh
-rwxr-xr-x 1 work work 3399 6月   7 01:35 workers.sh
```

- 启停集群

  在主节点：

  ```
  start-all.sh
  stop-all.sh
  ```

- 单独启停

  在主节点单独启动和停止master：

  ```
  start-master.sh
  stop-master.sh
  ```

  在从节点单独启动和停止worker:

  ```
  start-worker.sh
  stop-worker.sh
  ```

观察：

- `jps`

- master： http://node1:8080

- worker:  http://node2:8081

### 4. 测试

启动`spark-shell`：

```
bin/spark-shell --master spark://node1:7077
```

Web UI: http://node1:4040



默认端口总结:

- spark
  - 4040：任务运行web ui界面端口
  - 8080：spark集群web ui界面端口
  - 7077：提交任务时的通信端口
- hadoop
  - 50070：集群web ui界面端口
  - 8020/9000(老版本)：文件上传下载通信端口



# 三、Spark On YARN

## 1. 原理

Spark On YARN的本质是把Spark任务的字节码文件打成jar包，上传到YARN集群的JVM中运行。同时Spark集群的相关角色也会在YARN的JVM中运行。

YARN是集群的资源管理系统，有几个概念：

- `ResourceManager`：负责整个集群的资源管理和分配。
- `NodeManager`：每个节点的资源和任务管理器，负责启动/停止Container，并监视资源使用情况。可以理解为集群的每一台slave。
- `ApplicationMaster`：YARN中每个Application对应一个AM进程，负责向RM申请资源，获取资源后告诉`NodeManager`为其分配并启动`Container`。(资源其实就是`Container`，`Container`可以理解为cpu cores、memory)

- `Container`：YARN中的抽象资源。

## 2. 运行模式

Spark On YARN由两种运行模式：

- cluster

  在yarn上启动一个appmaster作为Driver，完成`DAGscheduler`和`TaskScheduler`，安装Spark的机器是「仅上传jar包到HDFS的角色」。

- client

  Driver程序在本地机器中运行。

### a. cluster模式

```shell
 ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 1g \
    --executor-memory 1g \
    --executor-cores 2 \
    --queue default \
    lib/spark-examples*.jar \
    10
```

<img src="./pics/yarn-cluster启动流程.jpeg" style="zoom:67%;" />

Spark Driver首先作为一个`ApplicationMaster`在YARN集群中启动，客户端提交给`ResourceManager`的每一个job都会在集群的`NodeManager`节点上分配一个唯一的`ApplicationMaster`，由该`ApplicationMaster`管理全生命周期的应用。具体过程：

1. 由client向`ResourceManager`提交请求，并上传jar到HDFS上。这期间包括四个步骤：
   - 连接到RM 
   - 从RM的ASM（ApplicationsManager）中获得metric、queue和resource等信息。 
   - upload app jar and spark-assembly jar 
   - 设置运行环境和container上下文（launch-container.sh等脚本)
2. `ResouceManager`向`NodeManager`申请资源，创建 `ApplicationMaster`（每个`SparkContext`都有一个`ApplicationMaster`）
3. `NodeManager`启动`ApplicationMaster`，并向`ResourceManager ASM` 注册
4. `ApplicationMaster`从HDFS中找到jar文件，启动`SparkContext`、`DAGscheduler`和YARN Cluster Scheduler
5. `ResourceManager`向`ResourceManager ASM`注册申请container资源
6. `ResourceManager`通知`NodeManager`分配container，这时可以收到来自ASM关于container的报告。（每个container对应一个executor）
7. Spark `ApplicationMaster`直接和container（executor）进行交互，完成这个分布式任务。

### b. client模式

```shell
./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode client \
    --driver-memory 1g \
    --executor-memory 1g \
    --executor-cores 2 \
    --queue default \
    lib/spark-examples*.jar \
    10
```

在client模式下，Driver运行在Client上，通过`ApplicationMaster`向RM获取资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。

<img src="./pics/yarn-client启动流程.jpeg" style="zoom:67%;" />

客户端的Driver将应用提交给Yarn后，Yarn会先后启动`ApplicationMaster`和executor，另外`ApplicationMaster`和executor都是装载在container里运行，container默认的内存是1G，`ApplicationMaster`分配的内存是driver-memory，executor分配的内存是executor-memory。Driver以进程名为`SparkSubmit`的形式存在。

