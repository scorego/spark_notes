[toc]

# Building Spark

简单翻译官方文档： http://spark.apache.org/docs/latest/building-spark.html

(本文是3.1.1版本)

# 一、 Building

## 1.1 使用Maven编译

使用Maven编译是Spark的参考构建方式。要求Maven版本3.6.3。

### a. 设置Maven Memory Usage参数

```
export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=1g"
```

如果不设置这个参数，可能会报如下错误：

```
[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.12/classes...
[ERROR] Java heap space -> [Help 1]
```

### b. build/mvn

Spark现在自带了Maven安装脚本，脚本会在`build/`目录自动下载安装并安装所有依赖（比如Mavenn、Scala、Zinc），且这个脚本下载的是自己的备份以确保版本正确。`build/mvn`是调用方式，比如可以用以下命令构建spark：

```
./build/mvn -DskipTests clean package
```

## 1.2 编译一个可运行的发布

使用`./dev/make-distribution.sh`来创建一个Spark distribution。可以像直接使用Maven一样配置Maven的参数，如：

```
./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes
```

更多信息可参考`./dev/make-distribution.sh --help`。

## 1.3 指定Hadoop版本 & 启用YARN

因为 HDFS 各版本协议是不兼容的，如果想从 HDFS 中读取数据，需要编译 Spark 来适应具体的 HDFS 版本。可以通过`hadoop.version`参数来指定Hadoop版本。

可以通过`yarn`来启动YARN支持，如果YARN版本和Hadoop版本不一致，可通过`yarn.version`参数来设置。

如：

```
./build/mvn -Pyarn -Dhadoop.version=2.8.5 -DskipTests clean package

mvn -Pyarn -Dhadoop.version=2.7.0 -DskipTests clean package
mvn -Pyarn -Dhadoop.version=2.7.0 -Dyarn.version=2.4.0 -DskipTests clean package
```

## 1.4 Hive和JDBC支持

如果开启带 Hive 整合以及 JDBC 服务器和命令行界面 (CLI) 支持的 Spark SQL，添加`-Phive`和`-Phive-thriftserver`配置参数到现有的编译选项中。默认Spark会使用Hive2.3.7编译。

```
# With Hive 2.3.7 support
./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package
```

## 1.5 Packaging without Hadoop Dependencies for YARN

The assembly directory produced by `mvn package` will, by default, include all of Spark’s dependencies, including Hadoop and some of its ecosystem projects. On YARN deployments, this causes multiple versions of these to appear on executor classpaths: the version packaged in the Spark assembly and the version on each node, included with `yarn.application.classpath`. The `hadoop-provided` profile builds the assembly without including Hadoop-ecosystem projects, like ZooKeeper and Hadoop itself.

## 1.6 Mesos支持

```
./build/mvn -Pmesos -DskipTests clean package
```

## 1.7 Kubernetes支持

```
./build/mvn -Pkubernetes -DskipTests clean package
```

## 1.8 独立编译子模块

可以使用`mvn -pl`选项来编译Spark子模块。比如，编译Spark Streaming：

```
./build/mvn -pl :spark-streaming_2.12 clean install
```

这里的`spark-streaming_2.12`是`streaming/pom.xml`文件中的`artifactId`。

## 1.9 持续编译 (Continuous Compilation)

使用 `scala-maven-plugin`插件支持渐进和持续编译。如：

```
./build/mvn scala:cc
```

将进行持续编译（例如随时监测代码变化，一有改变就编译(wait for changes)）。然而，这个并没有广泛测过。会有一些坑：

- 它只扫描`src/main`和`src/test`路径（可查看 [docs](http://scala-tools.org/mvnsites/maven-scala-plugin/usage_cc.html)），所以它只会在具有这个结构的子模块下工作。
- 你将需要运行 mvn install 从项目根目录下编译到在具体子模块中来工作。这是因为子模块通过 spark-parent 模块依赖其他子模块

所以，完整的运行`core`子模块连续-编译的代码段 可能更像下面这段：

```
$ ./build/mvn install
$ cd core
$ ../build/mvn scala:cc
```

## 1.10 使用SBT编译

SBT在不断更新发展，这是因为它能提供更快的迭代编译。更多高级的开发者可能希望使用 SBT。

SBT 编译是源自 Maven POM 文件，使用相同的 Maven 配置和变量同样可以控制SBT编译。如：

```
./build/sbt package
```

可以在项目的根目录的`.jvmopts`为SBT设置JVM选项，如内存：

````
-Xmx2g
-XX:ReservedCodeCacheSize=1g
````

## 1.11 加速编译

参考：https://spark.apache.org/developer-tools.html#reducing-build-times

## 1.12 加密的文件系统

在一个加密的文件系统上（比如home目录被加密了）编译可能会失败，报错信息形如`Filename too long`。作为一个变通方案，将下面添加到 项目 project`pom.xml`中的`scala-maven-plugin` 的配置参数：

```
<arg>-Xmax-classfile-name</arg>
<arg>128</arg>
```

并在`project/SparkBuild.scala`中的`sharedSettings`变量添加：

```
scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"),
```

## 1.13 IntelliJ IDEA还是Eclispse

参考：https://spark.apache.org/developer-tools.html#reducing-build-times

# 二、执行测试

通过 [ScalaTest Maven plugin](http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin)默认执行Test。需要使用root或admin用户来执行测试。

测试命令如：

```
./build/mvn test
```

## 2.1 Testing with SBT

example：

```
./build/sbt test
```

## 2.2 执行独立测试

参考：

## 2.3 PySpark pip install

如果使用Python环境编译并准备用pip来安装，首先需要编译上面所说的JAR包。然后：

```
cd python; python setup.py sdist
```

## 2.4 更换Scala版本

有些版本支持Scala2.13，可以使用下述命令更换Scala版本：

```
./dev/change-scala-version.sh 2.13
```

启用文件：

```
# For Maven
./build/mvn -Pscala-2.13 compile

# For sbt
./build/sbt -Pscala-2.13 compile
```

