# 一、 spark-shell

文件位置： `bin/spark-shell`

文件描述： Shell script for starting the Spark Shell REPL

该脚本核心处理就3步：

1. 为`${SPARK_HOME}`找到一个合适的值

   ```
   if [ -z "${SPARK_HOME}" ]; then
     source "$(dirname "$0")"/find-spark-home
   fi
   ```

2. `main`方法，调用`spark-submit`脚本

   ```
   function main() {
     if $cygwin; then
       stty -icanon min 1 -echo > /dev/null 2>&1
       export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"
       "${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"
       stty icanon echo > /dev/null 2>&1
     else
     	# 看这里
       export SPARK_SUBMIT_OPTS
       "${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"
     fi
   }
   
   main "$@"
   ```

3. 以`main`方法的返回值退出

   ```
   # record the exit status lest it be overwritten:
   # then reenable echo and propagate the code.
   exit_status=$?
   onExit
   ```



可以看到，`spark-shell`实际上就是执行了`spark-submit`脚本，和提交其他普通spark作业是一样的。调用`spark-submit`的时候传了3类参数。分别是：

- --class org.apache.spark.repl.Main
- --name "Spark shell"
- "$@"



# 二、 spark-submit

文件位置： `bin/spark-submit`

代码：

```
if [ -z "${SPARK_HOME}" ]; then
  source "$(dirname "$0")"/find-spark-home
fi

# disable randomized hash for string in Python 3.3+
export PYTHONHASHSEED=0

exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"
```

`spark-submit`脚本是通过执行`spark-class`来提交作业，而且给`spark-class`增加了参数`org.apache.spark.deploy.SparkSubmit`，由此可见，Spark启动了以`SparkSubmit`为主类的JVM进程。

# 三、 spark-class

文件位置： `bin/spark-class`

代码:

```
1. 确认${SPARK_HOME}变量被设置
if [ -z "${SPARK_HOME}" ]; then
  source "$(dirname "$0")"/find-spark-home
fi

2. load spark-env.sh
. "${SPARK_HOME}"/bin/load-spark-env.sh

3. 确认java路径，赋给RUNNER变量
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ "$(command -v java)" ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" >&2
    exit 1
  fi
fi

4. Find Spark jars
if [ -d "${SPARK_HOME}/jars" ]; then
  SPARK_JARS_DIR="${SPARK_HOME}/jars"
else
  SPARK_JARS_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars"
fi

if [ ! -d "$SPARK_JARS_DIR" ] && [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then
  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1>&2
  echo "You need to build Spark with the target \"package\" before running this program." 1>&2
  exit 1
else
  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"
fi

5. 
# Add the launcher build dir to the classpath if requested.
if [ -n "$SPARK_PREPEND_CLASSES" ]; then
  LAUNCH_CLASSPATH="${SPARK_HOME}/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"
fi

# For tests
if [[ -n "$SPARK_TESTING" ]]; then
  unset YARN_CONF_DIR
  unset HADOOP_CONF_DIR
fi

6. 读入参数
build_command() {
  "$RUNNER" -Xmx128m $SPARK_LAUNCHER_OPTS -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"
  printf "%d\0" $?
}

set +o posix
CMD=()
DELIM=$'\n'
CMD_START_FLAG="false"
while IFS= read -d "$DELIM" -r ARG; do
  if [ "$CMD_START_FLAG" == "true" ]; then
    CMD+=("$ARG")
  else
    if [ "$ARG" == $'\0' ]; then
      # After NULL character is consumed, change the delimiter and consume command string.
      DELIM=''
      CMD_START_FLAG="true"
    elif [ "$ARG" != "" ]; then
      echo "$ARG"
    fi
  fi
done < <(build_command "$@")

7. 执行
COUNT=${#CMD[@]}
LAST=$((COUNT - 1))
LAUNCHER_EXIT_CODE=${CMD[$LAST]}
CMD=("${CMD[@]:0:$LAST}")
exec "${CMD[@]}"
```

